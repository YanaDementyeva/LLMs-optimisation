{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3TfDYjTzgnWU",
        "xa13XbHV5rft",
        "sOWOpFA5T61o",
        "UasOB8rfAZaW"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preparing"
      ],
      "metadata": {
        "id": "3TfDYjTzgnWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lxml import etree\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "\n",
        "def load_sentirueval_2016(file_name: str) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"Load texts and labels from SentiRuEval 2016 XML file.\"\"\"\n",
        "    root = _read_xml_file(file_name)\n",
        "    database = _find_first_database(root)\n",
        "    texts, labels = _process_database_tables(database, file_name)\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "def _read_xml_file(file_name: str) -> etree._Element:\n",
        "    with open(file_name, mode='rb') as fp:\n",
        "        xml_data = fp.read()\n",
        "    return etree.fromstring(xml_data)\n",
        "\n",
        "\n",
        "def _find_first_database(root: etree._Element) -> etree._Element:\n",
        "    \"\"\"Find first database element in XML tree.\"\"\"\n",
        "    for element in root.getchildren():\n",
        "        if element.tag == 'database':\n",
        "            return element\n",
        "    raise ValueError('No database element found in XML')\n",
        "\n",
        "\n",
        "def _process_database_tables(database: etree._Element, file_name: str) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"Process all tables in database and extract texts and labels.\"\"\"\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    for table in database.getchildren():\n",
        "        if table.tag != 'table':\n",
        "            continue\n",
        "\n",
        "        text, label = _process_table_columns(table)\n",
        "        _validate_table_data(text, label, file_name)\n",
        "\n",
        "        texts.append(text)\n",
        "        labels.append(label)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "def _process_table_columns(table: etree._Element) -> Tuple[Optional[str], Optional[str]]:\n",
        "    \"\"\"Extract text and label from table columns.\"\"\"\n",
        "    text = None\n",
        "    label = None\n",
        "\n",
        "    for column in table.getchildren():\n",
        "        column_name = column.get('name')\n",
        "        column_value = str(column.text).strip() if column.text else ''\n",
        "\n",
        "        if column_name == 'text':\n",
        "            text = column_value\n",
        "        elif column_name not in {'id', 'twitid', 'date'}:\n",
        "            label = _parse_label(column_value)\n",
        "            # if column_value in ['1', '0', '-1']:\n",
        "            #     label = column_value\n",
        "            # elif column_value == 'NULL':\n",
        "            #     label = '0'\n",
        "\n",
        "        if text is not None and label is not None:\n",
        "            break\n",
        "\n",
        "    return text, label\n",
        "\n",
        "\n",
        "def _parse_label(label_str: str) -> Optional[str]:\n",
        "    \"\"\"Convert numeric label to textual representation.\n",
        "       Not nessesary for tf models\"\"\"\n",
        "    if label_str == '-1':\n",
        "        return 'negative'\n",
        "    elif label_str == '1':\n",
        "        return 'positive'\n",
        "    elif label_str == '0':\n",
        "        return 'neutral'\n",
        "    return None\n",
        "\n",
        "def _validate_table_data(text: Optional[str], label: Optional[str], file_name: str) -> None:\n",
        "    \"\"\"Validate that both text and label were found.\"\"\"\n",
        "    if text is None or label is None:\n",
        "        raise ValueError(f'File `{file_name}` contains incomplete data (text or label missing)')\n",
        "\n",
        "\n",
        "texts, labels = load_sentirueval_2016('bank_train_2016.xml')"
      ],
      "metadata": {
        "id": "ZU8xdaWn28Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "-YnA5jBMhBfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_labels(labels, label_map):\n",
        "    new_labels = []\n",
        "    for label in labels:\n",
        "        new_label = label_map[label]\n",
        "        new_labels.append(new_label)\n",
        "    return new_labels\n",
        "\n",
        "label_map = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
        "labels = convert_labels(labels, label_map)\n",
        "labels = to_categorical(labels, num_classes=3) # для pt не использовать"
      ],
      "metadata": {
        "id": "MoaeAhCCg8iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "t_texts, test_texts, t_labels, test_labels = train_test_split(texts, labels, test_size=0.2, stratify=labels, random_state=2025)\n",
        "train_texts, dev_texts, train_labels, dev_labels = t_texts[:5263], t_texts[5263:], t_labels[:5263], t_labels[5263:]\n",
        "\n",
        "test_samples = [(text, label) for text, label in zip(test_texts, test_labels)]\n",
        "train_samples = [(text, label) for text, label in zip(train_texts, train_labels)]\n",
        "dev_samples = [(text, label) for text, label in zip(dev_texts, dev_labels)]"
      ],
      "metadata": {
        "id": "wB9UHhhGg9c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of TRAIN texts is {0}, number of labels is {1}.'.format(len(train_texts), len(train_labels)))\n",
        "print('Number of VAL texts is {0}, number of labels is {1}.'.format(len(dev_texts), len(dev_labels)))\n",
        "print('Number of TEST texts is {0}, number of labels is {1}.'.format(len(test_texts), len(test_labels)))"
      ],
      "metadata": {
        "id": "aUlPDil9hPj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classes for tf-model"
      ],
      "metadata": {
        "id": "G9cPTsgsTwdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing model"
      ],
      "metadata": {
        "id": "pWSsbIdqhz0t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLBEm1hTDFxT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "from transformers import BertModel, BertConfig, BertForSequenceClassification,\n",
        "                          TFBertForTokenClassification, TFBertForSequenceClassification, BertTokenizer\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model_name):\n",
        "    if model_name == 'tf_bert_for_sequence_classification':\n",
        "        config = BertConfig.from_pretrained('DeepPavlov/rubert-base-cased', from_pt=True, num_labels=3)\n",
        "        tf_model = TFBertForSequenceClassification.from_pretrained(\n",
        "            \"DeepPavlov/rubert-base-cased\",\n",
        "            from_pt=True,\n",
        "            config=config\n",
        "        )\n",
        "        #tf_model.layers[-1].activation = tf.keras.activations.softmax\n",
        "        #tf_model.layers[-1].trainable = True\n",
        "        return tf_model"
      ],
      "metadata": {
        "id": "oqHx_A-5ogrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "5QqzGq61X7mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_model = get_model('tf_bert_for_sequence_classification')\n",
        "# tf_model.load_weights('/content/drive/MyDrive/weights/senti_classification_3_weights.h5')\n",
        "tf_model.load_weights('/content/drive/MyDrive/weights/RUbert_cross_domain_main_weights.h5')\n",
        "tf_model.summary()"
      ],
      "metadata": {
        "id": "Ryq52F94MvF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как работает `TFBertForSequenceClassification`:\n",
        "\n",
        "* На выходе BERT для классификации последовательностей обычно используется специальный токен [CLS], который агрегирует информацию обо всей последовательности.\n",
        "Вектор, соответствующий этому токену, передаётся в классификатор.\n",
        "\n",
        "* Слой `tf.keras.layers.Dense`, который добавляется поверх выходных представлений BERT. Он имеет размерность (hidden_size, num_labels) и принимает на вход вектор [CLS] (размерности hidden_size) и преобразует его в вектор размерности num_labels (в данном случае 3).\n",
        "\n",
        "* На выходе применяется функция softmax, которая преобразует логиты (сырые выходы Dense-слоя) в вероятности классов."
      ],
      "metadata": {
        "id": "vI0d9dGNRlel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### evaluation full models"
      ],
      "metadata": {
        "id": "XYOMABJsNA14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Tuple, List\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "tte8iYvZJz-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vectorizer:\n",
        "\n",
        "    def __init__(self):\n",
        "        self._tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased',\n",
        "                                                        do_lower_case=False)\n",
        "        self._max_length = 128\n",
        "\n",
        "    def vectorize(self, text: str, label: Any) -> Tuple[List[str], List[int], List[int], np.ndarray]:\n",
        "        try:\n",
        "            tokenized_text, input_masks = self._tokenize(text)\n",
        "        except:\n",
        "            tokenized_text, input_masks = [], []\n",
        "\n",
        "        input_ids = self._tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "        if isinstance(label, np.ndarray):\n",
        "          tag = label\n",
        "        else:\n",
        "          print('Here is the error!')\n",
        "\n",
        "        input_ids = self._pad(input_ids)\n",
        "        input_masks = self._pad(input_masks)\n",
        "\n",
        "        return tokenized_text, input_ids, input_masks, tag\n",
        "\n",
        "    def _pad(self, input: List[Any]) -> List[Any]:\n",
        "        if len(input) >= self._max_length:\n",
        "            return input[:self._max_length]\n",
        "        while len(input) < self._max_length:\n",
        "            input.append(0)\n",
        "        return input\n",
        "\n",
        "    def _tokenize(self, text: str) -> Tuple[List[str], List[int]]:\n",
        "        tokenized_text = self._tokenizer.tokenize(text)\n",
        "\n",
        "        inputs = self._tokenizer.encode_plus(\n",
        "            tokenized_text,\n",
        "            is_pretokenized=True,\n",
        "            return_attention_mask=True,\n",
        "            max_length=self._max_length,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return tokenized_text, inputs['attention_mask']"
      ],
      "metadata": {
        "id": "SnAW1sdvJwvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = Vectorizer()"
      ],
      "metadata": {
        "id": "18KNk8Zo5VRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "class Evaluator_tf:\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self._vectorizer = Vectorizer()\n",
        "        self._model = model\n",
        "        self._model.training = False\n",
        "        self._batch_size = 12\n",
        "\n",
        "    def _predict(self, texts):\n",
        "        all_labels = []\n",
        "\n",
        "        for i in range(0, len(texts), self._batch_size):\n",
        "            batch_texts = texts[i:i + self._batch_size]\n",
        "\n",
        "            X_ids = []\n",
        "            X_masks = []\n",
        "            for text in batch_texts:\n",
        "                _, input_ids, input_masks, _ = self._vectorizer.vectorize(text, np.zeros([1, 3]))\n",
        "                X_ids.append(np.array(input_ids))\n",
        "                X_masks.append(np.array(input_masks))\n",
        "\n",
        "            input = [np.asarray(X_ids, dtype='int32'), np.asarray(X_masks, dtype='int32')]\n",
        "            # print(\"Tensorflow inpyt:\", input)\n",
        "            preds = self._model(input)[0]\n",
        "            # print(\"Tensorflow output:\", self._model(input))\n",
        "            max_index = np.argmax(preds, axis=1)\n",
        "            label_one_hot = np.eye(3)[max_index]\n",
        "            all_labels.extend(label_one_hot)\n",
        "\n",
        "        return np.array(all_labels)\n",
        "\n",
        "    def eval(self, test_texts, test_labels):\n",
        "        all_y_true = test_labels\n",
        "        all_y_pred = self._predict(test_texts)\n",
        "\n",
        "        report = classification_report(all_y_true, all_y_pred, target_names=['negative', 'neutral', 'positive'])\n",
        "        return report"
      ],
      "metadata": {
        "id": "IyxBAXIDTEDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = Evaluator_tf(tf_model)\n",
        "print(evaluator.eval(test_texts, test_labels))"
      ],
      "metadata": {
        "id": "0djrxwv6XkSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### get model structure"
      ],
      "metadata": {
        "id": "xa13XbHV5rft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Отображалка имен и размерностей весов на слоях\n",
        "\n",
        "def print_layer_info(layer):\n",
        "    print(f\"Layer: {layer.name} ({layer.__class__.__name__})\")\n",
        "\n",
        "    if hasattr(layer, 'weights') and layer.weights:\n",
        "        for i, weight in enumerate(layer.weights):\n",
        "            print(f\"  Weight {i}: {weight.name}, Shape: {weight.shape}\")\n",
        "\n",
        "model = pretrained_model\n",
        "for layer in model.layers:\n",
        "    print_layer_info(layer)"
      ],
      "metadata": {
        "id": "LJ4ZuE8cPsvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization v1\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/compat/v1/quantization"
      ],
      "metadata": {
        "id": "sM4o3nwAqXbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy"
      ],
      "metadata": {
        "id": "2-VWdMWkW4qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "YsRlpreVW4qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = copy.deepcopy(tf_model)"
      ],
      "metadata": {
        "id": "ksDF9o8WW4qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.v1 as tf"
      ],
      "metadata": {
        "id": "rDVhlR2vrTfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_weights(layer):\n",
        "    \"\"\"\n",
        "    Квантизирует и де-квантизирует веса слоя.\n",
        "    \"\"\"\n",
        "    # print(layer)\n",
        "    new_weights = []\n",
        "    for weight in layer.get_weights():\n",
        "        weight_tensor = tf.convert_to_tensor(weight, dtype=tf.float32)  # выглядят так tf.Tensor(-0.6931182, shape=(), dtype=float32)\n",
        "\n",
        "        min_range = tf.reduce_min(weight_tensor)\n",
        "        max_range = tf.reduce_max(weight_tensor)\n",
        "\n",
        "        quantized_output = tf.quantization.quantize(\n",
        "            weight_tensor,\n",
        "            min_range=min_range,\n",
        "            max_range=max_range,\n",
        "            T=tf.qint8,\n",
        "            mode=\"SCALED\",\n",
        "            round_mode=\"HALF_AWAY_FROM_ZERO\", # An optional string from: \"HALF_AWAY_FROM_ZERO\", \"HALF_TO_EVEN\". Defaults to \"HALF_AWAY_FROM_ZERO\".\n",
        "            narrow_range=False,\n",
        "            # axis=-1,\n",
        "            ensure_minimum_range=0.1 # 0.001 дало меньшие изменения\n",
        "        )\n",
        "\n",
        "        # Де-квантизация обратно в float32 нужна, чтобы модель конфигурировалась и работала\n",
        "        dequantized_weight = tf.quantization.dequantize(\n",
        "            quantized_output[0],\n",
        "            quantized_output[1],\n",
        "            quantized_output[2],\n",
        "            mode=\"MIN_COMBINED\",  # An optional string from: \"MIN_COMBINED\", \"MIN_FIRST\", \"SCALED\". Defaults to \"MIN_COMBINED\".\n",
        "            narrow_range=False,\n",
        "            # axis=-1,\n",
        "            dtype = tf.bfloat16\n",
        "        )\n",
        "\n",
        "        new_weights.append(dequantized_weight.numpy())\n",
        "\n",
        "    layer.set_weights(new_weights)"
      ],
      "metadata": {
        "id": "h5M6Jlad8X8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_quantization_to_model(model):\n",
        "    if hasattr(model, 'bert') and hasattr(model.bert, 'encoder'):\n",
        "        encoder = model.bert.encoder\n",
        "        for layer in encoder.layer:\n",
        "            # attention\n",
        "            if hasattr(layer, 'attention'):\n",
        "                attention = layer.attention.self_attention\n",
        "                # query, key, value\n",
        "                if hasattr(attention, 'query'):\n",
        "                    quantize_weights(attention.query)\n",
        "                if hasattr(attention, 'key'):\n",
        "                    quantize_weights(attention.key)\n",
        "                if hasattr(attention, 'value'):\n",
        "                    quantize_weights(attention.value)\n",
        "                # output dense\n",
        "            if hasattr(layer.attention, 'output') and hasattr(layer.attention.output, 'dense'):\n",
        "                    quantize_weights(attention.output.dense)\n",
        "            # intermediate dense\n",
        "            if hasattr(layer, 'intermediate') and hasattr(layer.intermediate, 'dense'):\n",
        "                quantize_weights(layer.intermediate.dense)\n",
        "            # output dense\n",
        "            if hasattr(layer, 'output') and hasattr(layer.output, 'dense'):\n",
        "                quantize_weights(layer.output.dense)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "mYy-1wH6sup5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apply_quantization_to_model(quantized_model)"
      ],
      "metadata": {
        "id": "O3jMM4Iwza-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model.save_weights('rubert_ner_quantized_bfloat16.h5')"
      ],
      "metadata": {
        "id": "jc_d0niEq-Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Квантизация с новым модулем tf.quantization.experimental\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/quantization/experimental"
      ],
      "metadata": {
        "id": "c4JCZvZyCyxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(tf_model, '/tmp/input_model')"
      ],
      "metadata": {
        "id": "bsNkq9IvCy_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
        "\n",
        "def create_representative_dataset(texts, tokenizer, num_samples=256):\n",
        "    max_length = 128\n",
        "    representative_samples = []\n",
        "\n",
        "    for text in texts[:num_samples]:\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "\n",
        "        representative_samples.append({\n",
        "            'input_ids': inputs['input_ids'],\n",
        "            'attention_mask': inputs['attention_mask'],\n",
        "            'token_type_ids': inputs['token_type_ids']\n",
        "        })\n",
        "\n",
        "    return representative_samples\n",
        "\n",
        "representative_samples = create_representative_dataset(dev_texts, tokenizer)\n",
        "\n",
        "quantization_options = tf.quantization.experimental.QuantizationOptions(\n",
        "    signature_keys=['serving_default'],\n",
        "    min_num_elements_for_weights=1024\n",
        ")\n",
        "\n",
        "tf.quantization.experimental.quantize_saved_model(\n",
        "    '/tmp/input_model',\n",
        "    '/tmp/output_model',\n",
        "    quantization_options=quantization_options,\n",
        "    representative_dataset={'serving_default': representative_samples},\n",
        ")"
      ],
      "metadata": {
        "id": "oynnZgjRH43v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ОЗУ не хватает, значит, попробуем делить на потоки (upd: cuda не зашла)"
      ],
      "metadata": {
        "id": "h3WmfP1SLLLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer\n",
        "import gc\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')\n",
        "\n",
        "def representative_dataset_generator(texts, tokenizer, num_samples=128):\n",
        "    max_length = 128\n",
        "\n",
        "    for text in texts[:num_samples]:\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            max_length=max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "\n",
        "        yield {\n",
        "            'input_ids': inputs['input_ids'],\n",
        "            'attention_mask': inputs['attention_mask'],\n",
        "            'token_type_ids': inputs['token_type_ids']\n",
        "        }\n",
        "\n",
        "        # Очистка памяти после каждого образца\n",
        "        del inputs\n",
        "        tf.keras.backend.clear_session()\n",
        "        gc.collect()\n",
        "\n",
        "# Использование генератора напрямую (без загрузки всех данных в память)\n",
        "quantization_options = tf.quantization.experimental.QuantizationOptions(\n",
        "    signature_keys=['serving_default'],\n",
        "    min_num_elements_for_weights=1024\n",
        ")\n",
        "\n",
        "tf.quantization.experimental.quantize_saved_model(\n",
        "    '/tmp/input_model',\n",
        "    '/tmp/output_model',\n",
        "    quantization_options=quantization_options,\n",
        "    representative_dataset={'serving_default': representative_dataset_generator(texts, tokenizer)},\n",
        ")"
      ],
      "metadata": {
        "id": "v7utvClbLScp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### time eval & save model"
      ],
      "metadata": {
        "id": "wYlBo5NbLVB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "X_ids = []\n",
        "X_masks = []\n",
        "for text in test_texts[:50]:\n",
        "    _, input_ids, input_masks, _ = vectorizer.vectorize(text, np.zeros([1, 3]))\n",
        "    X_ids.append(np.array(input_ids))\n",
        "    X_masks.append(np.array(input_masks))\n",
        "\n",
        "input_data = [np.asarray(X_ids, dtype='int32'), np.asarray(X_masks, dtype='int32')]\n",
        "X_ids = np.asarray(X_ids, dtype='int32')\n",
        "X_masks = np.asarray(X_masks, dtype='int32')\n",
        "\n",
        "for model in [tf_model, clustered_classification]:\n",
        "  preds = model(input_data)[0]\n",
        "  max_index = np.argmax(preds, axis=1)\n",
        "  label_one_hot = np.eye(3)[max_index]\n",
        "\n",
        "  execution_times = []\n",
        "\n",
        "  for i in range(50):\n",
        "      start_time = time.time()\n",
        "      model.predict([X_ids[i:i+1], X_masks[i:i+1]], verbose=0)\n",
        "      end_time = time.time()\n",
        "      execution_times.append(end_time - start_time)\n",
        "\n",
        "  # Вычисляем среднее время выполнения\n",
        "  average_execution_time = np.mean(execution_times)\n",
        "\n",
        "  print(f\"Среднее время выполнения predict: {average_execution_time:.4f} секунд\")"
      ],
      "metadata": {
        "id": "oYPOQxysXYUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "А для NER:"
      ],
      "metadata": {
        "id": "8ZvKWnN8qmcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "mmqeDXmrqh4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В таком виде модель, конечно, не занимает меньше места\n",
        "\n",
        "Поэтому сохраняем модель в формате TFLite с квантизацией"
      ],
      "metadata": {
        "id": "NJLblZefZvET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_tflite_model = converter.convert()\n",
        "\n",
        "with open('quantized_model.tflite', 'wb') as f:\n",
        "    f.write(quantized_tflite_model)"
      ],
      "metadata": {
        "id": "mTTjhiqqDtJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "original_size = os.path.getsize('my_rubert_quantized.h5')\n",
        "quantized_size = os.path.getsize('quantized_model.tflite')\n",
        "print(f\"Original model size: {original_size} bytes\")\n",
        "print(f\"Quantized model size: {quantized_size} bytes\")"
      ],
      "metadata": {
        "id": "AMoaXd8oatkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "квантование — это необратимое сжатие.\n",
        "\n",
        "Эти модели предназначены для выполнения инференса на устройствах с поддержкой TFLite\n",
        "\n",
        "Где работают квантованные модели?\n",
        "* int8/uint8:\n",
        "\n",
        "  * CPU/GPU с поддержкой INT8 (например, ARM NEON, x86 AVX2).\n",
        "\n",
        "  * Ускорители типа Google Edge TPU, NVIDIA TensorRT.\n",
        "\n",
        "* float16:\n",
        "\n",
        "  * GPU с поддержкой FP16 (мобильные GPU, NVIDIA).\n",
        "\n",
        "* float32:\n",
        "\n",
        "  * Любое устройство с поддержкой TFLite.\n",
        "\n",
        "(однако все ещё есть библиотека tflite2tensorflow)"
      ],
      "metadata": {
        "id": "AywTIR1OavbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "К изучению:\n",
        "\n",
        "см. туториал\n",
        "https://ai.google.dev/edge/litert/models/post_training_integer_quant\n",
        "\n",
        "и\n",
        "\n",
        "https://ai.google.dev/edge/litert/models/post_training_quantization\n",
        "\n",
        "Динамическая квантизация: Применяется с использованием `converter.optimizations = [tf.lite.Optimize.DEFAULT]`. Это квантует только веса модели.\n",
        "\n",
        "Полная квантизация: Для квантизации активаций используется representative_dataset, который предоставляет примеры входных данных для оценки диапазона значений активаций. Это позволяет квантовать и веса, и активации.\n",
        "\n",
        "Целочисленная квантизация: Дополнительно задаются параметры `converter.inference_input_type = tf.uint8` и `converter.inference_output_type = tf.uint8`, чтобы входные и выходные данные также были целочисленными."
      ],
      "metadata": {
        "id": "LOexOPeajf0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно визуализировать распределение весов до и после квантизации, чтобы увидеть, как изменились значения."
      ],
      "metadata": {
        "id": "LlJJ5UmtEQ0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "original_model = tf_model\n",
        "\n",
        "def plot_weight_distribution(weights, title):\n",
        "    plt.hist(weights.flatten(), bins=50, alpha=0.7)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Weight value\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "# Визуализация оригинальных весов\n",
        "for layer in original_model.layers:\n",
        "    if hasattr(layer, 'weights') and layer.weights:\n",
        "        weights = layer.get_weights()[0]\n",
        "        plot_weight_distribution(weights, f\"Original weights of {layer.name}\")\n",
        "\n",
        "# Визуализация квантизированных весов\n",
        "for layer in quantized_model.layers:\n",
        "    if hasattr(layer, 'weights') and layer.weights:\n",
        "        weights = layer.get_weights()[0]\n",
        "        plot_weight_distribution(weights, f\"Quantized weights of {layer.name}\")"
      ],
      "metadata": {
        "id": "A6bjWMQCEHcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yCwcEYIPENaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPTQ"
      ],
      "metadata": {
        "id": "UasOB8rfAZaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade accelerate optimum transformers"
      ],
      "metadata": {
        "id": "p0NigqQvCmAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gptqmodel --no-build-isolation"
      ],
      "metadata": {
        "id": "yJT3BYu7JBcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gptqmodel"
      ],
      "metadata": {
        "id": "AEDSIIOpJnPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertConfig, BertForSequenceClassification, BertTokenizer, GPTQConfig, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "S6AUeh_3AfAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.gptq import GPTQQuantizer\n",
        "# from auto_gptq import GPTQQuantizer"
      ],
      "metadata": {
        "id": "Y82GoHncCRXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing model"
      ],
      "metadata": {
        "id": "qMSb13fWAmzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch!!\n",
        "config = BertConfig.from_pretrained('DeepPavlov/rubert-base-cased', from_pt=True, num_labels=3)\n",
        "pt_model = BertForSequenceClassification.from_pretrained(\n",
        "            \"DeepPavlov/rubert-base-cased\",\n",
        "            config=config)"
      ],
      "metadata": {
        "id": "2AM6g7aTXpMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pt_model.config._attn_implementation = \"eager\""
      ],
      "metadata": {
        "id": "G-BwvsPXJJIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', do_lower_case=False)"
      ],
      "metadata": {
        "id": "RCeayo_zGZ7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "-jrs0vYqOqyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### gptquantization"
      ],
      "metadata": {
        "id": "gSSdpAeHHIQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gptq_config = GPTQConfig(\n",
        "    bits=4,\n",
        "    dataset=text_list,\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=8,\n",
        "    use_cuda=True,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "dNewezKvFlua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPTQConfig, BertLMHeadModel\n",
        "from optimum.gptq import GPTQQuantizer"
      ],
      "metadata": {
        "id": "LB_Jo6WeO7V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"DeepPavlov/rubert-base-cased\",\n",
        "    device_map=\"auto\",\n",
        "    max_memory={0: \"30GiB\", 1: \"46GiB\", \"cpu\": \"30GiB\"},\n",
        "    quantization_config=gptq_config\n",
        ")"
      ],
      "metadata": {
        "id": "m8vQlj7vP-KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertModel.from_pretrained(\n",
        "    \"DeepPavlov/rubert-base-cased\",\n",
        "    is_decoder=True\n",
        ")\n",
        "\n",
        "quantizer = GPTQQuantizer(\n",
        "    bits=4,\n",
        "    dataset=val_texts,\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=8,\n",
        "    # use_cuda=True,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "quantized_model = quantizer.quantize(model)"
      ],
      "metadata": {
        "id": "cK8Z9hydO16R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = BertModel.from_pretrained(\n",
        "    \"DeepPavlov/rubert-base-cased\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=gptq_config,\n",
        ")\n",
        "\n",
        "quantized_model.summary()"
      ],
      "metadata": {
        "id": "8tr7-n0kFy1g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}